# DEB_049

## Hadoop 탄생

* 참고: [하둡(Hadoop) 소개 및 기본 구성요소 설명](https://opentutorials.org/course/2908/17055)

* PC -> Internet -> Web -> Search Engine -> Mobile -> Cloud

* 더그 커팅

  <img src="https://data-flair.training/blogs/wp-content/uploads/sites/2/2020/02/History-and-Evolution-of-Hadoop.jpg" alt="map" style="zoom:50%;" />

  * Lucene: 오픈소스 검색 라이브러리
  * Apache Solr: 오픈소스 검색 프로젝트. request/response + Lucene
  * Apache Nutch: 웹 검색(indexing) 엔진 프로젝트. 웹사이트 크롤링
  * (구글의 GFS 논문 공개: 분산 저장과 분산 처리 개념)
  * NDFS (Nutch Distributed File System) 시작
  * Apache Hadoop 시작: 오픈소스형 Web 검색 엔진 & 분산 파일 시스템 프로젝트

## Hadoop 기술

* Apache Hadoop - HDFS (Hadoop Distributed File System)

  * 특징
    * 분산 저장
    * 분산 처리 (Map Reduce)
  * 구성요소
    * Name Node
    * Data Node
    * Clustering   

* 빅데이터 구현 기술

  | 수집        | 저장         | 처리            | 분석            | 표현      |
  | ----------- | ------------ | --------------- | --------------- | --------- |
  | 내부 데이터 | 정형데이터   | 일괄처리        | 전처리          | 보고서    |
  | 외부 데이터 | 비정형데이터 | 실시간처리/배치 | 데이터마크 구성 | 시각화    |
  | 크롤링      |              | 분산 병렬 처리  | 통계분석        | 분석 정보 |



## Big Data 프로젝트와 기술셋

bigdata 프로젝트를 진행할 때 고려해야할 네 가지 요소

1. 목적
   * Money+ / Money-
   * Data 수집 위치
2. 분석
   * 알아보는 것
   * 분석의 종류
     * 전통적인 방식: 샘플에서 전부(모수)를 알아가는 것
     * BigData 방식 : 가져다 놓은 Data 전체에서 속성(패턴)이 무엇인지 알아가는 것
   * 프로젝트 분석의 종류
     * 현황 분석: EDA
     * 진단 분석: 패턴 파악
     * 예측 분석: 예측
     * 최적화 분석
3. 구축
   * 구축 구성요소
     * Big Data를 제어 지식, 기술, 표현할 수 있는 사람
     * Data를 생성, 수집, 저장, 처리, 분석, 표현 기술
     * 3V + Value 를 충족하는 데이터
   * BigData 프로젝트 유형
     * Platform 구축 프로젝트: 설치
     * BigData 분석 프로젝트:  분석
     * BigData 운영 프로젝트: 유지
4. 구성 방식: 회사내부 System 구축, S/W Platform, IT service

## BigData 처리 과정

1. 수집: RDB, File, Web Crawling
   * Chukwa: 2008년도 Yahoo 에서 분산시스템의 로그 수집 및 모니터링, Hadoop 에 의존적
   * Flume: 2010년도 클라우데라에서 대량의 로그 데이터를 여러 소스에서 수집하여 저장하기 위한 목적으로 개발. Data 주입이 간단하고, 아키텍처가 유연
2. 저장: [CAP 이론](https://itwiki.kr/w/CAP_이론)
   * 대용량 파일 전체: HDFS
   * 대규모 메세지: NoSQL (HBase, MongoDB, Cassandra)
   * 인메모리 캐시: Redis
   * 대규모 메시지 데이터 버퍼링: Kafka
3. 처리
   * 데이터 처리
     * Pig: 2006년 Yahoo 에서 데이터 처리 언어 프레임워크로 개발
     * Hive: 2008년 Yahoo 에서  Hadoop 기반 SQL 언어로 개발
     * Sqoop: RDB, NoSQL DB에서 대용량 벌크 전송
     * Spark SQL
   * 워크플로
     * Oozie: 2009년 Yahoo 에서  Hadoop 기반 Workflow 제어 시스템
     * Yarn: 수 천개의 노드로 구성된 클러스터에서 작업이 제출되면 많은 작업들을 관리하고, 특정 작업에 사용할 자원(CPU, RAM)을 관리해주는 분산자원관리 기능을 담당
     * Spark: 분산 In-Memory Data 처리 Framework (SQL, Streaming, ML)
     * Zookeeper: 분산 시스템 간의 정보 공유 및 상태 체크, 동기화를 처리하는 프레임워크
