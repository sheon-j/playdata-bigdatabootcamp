# DEB_053



## 완전 분산 모드 : 네트워크

* 네트워크 설정
  * 파일 >> 호스트 네트워크 관리자 >> 만들기
  * [NN / DN 모두 적용] 어댑터 2 >> 호스트 전용 어댑터 >> 무작위 모드: 모두 허용

* 방화벽 내리기
  * systemctl stop firewalld.service
  * systemctl disable firewalld.service
* ip 구성
  * 설정 >> 네트워크 >> 이더넷(enp0s8) >> IPv4 >> 방식: 수동 / 주소: 아래 참조
    * NN01 : ip-192.168.56.101 / netmask-255.255.255.0
    * DN02: ip-192.168.56.102 / netmask-255.255.255.0
    * DN03: ip-192.168.56.103 / netmask-255.255.255.0
  * nmcli networking off : 네트워크 껐다
  * nmcli networking on : 네트워크 다시 키기 (새로고침)
  * ping 192.168.56.10x -c 3 : 연결 테스트

* 방화벽 내리기
  * systemctl stop firewalld.service
  * systemctl disable firewalld.service

---

## 네트워크 구성 설명

* 내부망을 위해 구축한 환경
  * hostname 변경
  * /etc/hosts 변경
  * Firewall (방화벽) 중단
  * SELinux (정책) 중단

---

## 키 설정

* root 계정(NN/DN)
  * ssh-keygen -t rsa
* hadoop 계정(NN/DN)
  * ssh-keygen -t rsa
  * rm ~/.ssh/authorized_keys
  * cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
  * chmod 640 ~/.ssh/authorized_keys
  * ssh localhost
  * exitc
* root 계정(NN) 키 뿌리기
  * rsa 키 경로: /home/hadoop/.ssh/id_rsa.pub
  *   ssh-copy-id -i /home/hadoop/.ssh/id_rsa.pub hadoop@hadoop02
  * ssh-copy-id -i /home/hadoop/.ssh/id_rsa.pub hadoop@hadoop03

---

## SSH / FTP

* 이것이 리눅스다 468 p.(ssh) 669 p.(ftp) 참고
* dnf info openssh : 설치 되어있는지 확인
* systemctl status sshd : 실행되는지 확인
* ssh localhost : 접속
  * ssh hadoop02 : hadoop02 접속
  * ssh hadoop03 : hadoop03 접속
* 키 복제
  * ssh-copy-id -i /home/hadoop/.ssh/id_rsa.pub hadoop@hadoop02
  * ssh-copy-id -i /home/hadoop/.ssh/id_rsa.pub hadoop@hadoop03
* 호스트OS에서 접속하기
  * 윈도우는 putty 프로그램을 쓰지만
  * **mac은 iterm 에서 ssh root@192.168.56.101 접속 가능**

---

## 환경설정

* $HADOOP_HOME/etc/hadoop/core-site.xml (NN/DN 공통)

  ```xml
  <configuration>
      <property>
          <name>fs.defaultFS</name>
          <value>hdfs://hadoop01:9000</value>
      </property>
  </configuration>
  ```

* `cat hadoop-env.sh | grep "export JAVA_HOME="` (NN/DN)

* hdfs-site.xml

  ```xml
  <!--NN-->
  <configuration>
  
    <property>
      <name>dfs.replication</name>
      <value>2</value>
    </property>
  
    <property>
      <name>dfs.name.dir</name>
      <value>file:///home/hadoop/hadoop/hdfs/namenode</value>
    </property>
  
    <property>
      <name>dfs.data.dir</name>
      <value>file:///home/hadoop/hadoop/hdfs/datanode</value>
    </property>
  
  </configuration>
  ```

  ```xml
  <!--DN-->
  <configuration>
  
    <property>
      <name>dfs.replication</name>
      <value>2</value>
    </property>
  
    <property>
      <name>dfs.data.dir</name>
      <value>file:///home/hadoop/hadoop/hdfs/datanode</value>
    </property>
  
  </configuration>
  ```

* mapred-site.xml

  ```xml
  <configuration>
  
    <property>
      <name>mapreduce.framework.name</name>
      <value>yarn</value>
    </property>
  
  </configuration>
  ```

* yarn-site.xml

  ```xml
  <configuration>
  
    <property>
      <name>yarn.nodemanager.aux-services</name>
      <value>mapreduce_shuffle</value>
    </property>
    
    <property>
      <name>yarn.nodemanager.aux-services.mapreduce_shuffle.class</name>
      <value>org.apache.hadoop.mapred.ShuffleHandler</value>
    </property>
    
  </configuration
  ```

---

## 실행

* hdfs namenode -formate (NN)
* start-dfs.sh (NN, DN)
* start-yarn.sh (NN, DN)
* 접속
  * 192.168.56.101:9000
  * 192.168.56.101:9870

```
[ hadoop01 ]
hdfs namenode -format
start-dfs.sh
jps 

  [ hadoop02 ~ 03 ]
start-dfs.sh
jps 

  [ hadoop01 ~ 03 ]
start-yarn.sh
jps
```

