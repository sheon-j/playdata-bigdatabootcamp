# DEB_051

## 참고사이트

* [Mac OS에 하둡(Hadoop) 설치](https://key4920.github.io/p/mac-os에-하둡hadoop-설치/)
* [macOS에서 Hadoop 설치하기](https://rap0d.github.io/tip/2019/10/01/mac_hadoop_in_mac/)

## 프로젝트 폴더 생성

* `mkdir ~/hadoop-workspace`

  ```
  -+= hadoop-workspace
   \-+= data
     |--= datanode
     |--= namenode
     |--= namesecondary
     \--= tmp
  ```

---

## 하둡 설치

* 버젼: jdk 1.8+ / Hadoop v3.2.2

* `brew install hadoop`

  * `hadoop version`: 3.1.1
  * 경로: `/usr/local/Cellar/hadoop/3.3.1`

* 환경 변수 설정

  * `open ~/.zshrc`

    ```
    HADOOP_HOME=/usr/local/Cellar/hadoop/3.3.1/libexec
    
    export PATH=${PATH}:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin
    ```

---

## 환경 설정

* 경로: `/usr/local/Cellar/hadoop/3.3.1/libexec/etc/hadoop`

* 수정 파일 목록

  > #1 hadoop-env.sh
  > #2 core-site.xml
  > #3 hdfs-site.xml
  > #4 mapred-site.xml
  > #5 yarn-site.xml

* #1 **hadoop-env.sh**: export 변수 추가

```shell
export HADOOP_OPTS="$HADOOP_OPTS -Djava.net.preferIPv4Stack=true -Djava.security.krb5.realm= -Djava.security.krb5.kdc="

export JAVA_HOME="/Library/Java/JavaVirtualMachines/adoptopenjdk-8.jdk/Contents/Home"
```



* #2 **core-site.xml**: configuration 추가

```xml
<property>
  <name>hadoop.tmp.dir</name>
  <value>/usr/local/Cellar/hadoop/hdfs/tmp</value>
  <description>A base for other temporary directories.</description>
</property>
<property>
  <name>fs.default.name</name>
  <value>hdfs://localhost:9870</value>
</property>
```



* #3 **hdfs-site.xm**l: configuration 추가

```xml
<property>
	<name>dfs.replication</name>
	<value>1</value>
</property>

<property>
	<name>dfs.namenode.name.dir</name>
	<value>file:///C:/HadoopPrj/hadoop-3.2.2/data/namenode</value>
</property>

<property>
	<name>dfs.datanode.data.dir</name>
	<value>file:///C:/HadoopPrj/hadoop-3.2.2/data/datanode</value>
</property>

<property>
	<name>dfs.namenode.checkpoint.dir</name>
	<value>file:///C:/HadoopPrj/hadoop-3.2.2/data/namesecondary</value>
</property>

<property>
  	<name>dfs.namenode.http-address</name>
  	<value>localhost:9860</value>
</property>

<property>
  <name>dfs.namenode.secondary.http-address</name>
  <value>localhost:9850</value>
</property>
```



* #4 **mapred-site.xml**: configuration 추가

```xml
<property>
  <name>yarn.resourcemanager.hostname</name>
  <value>127.0.0.1</value>
</property>

<property>
  <name>mapreduce.framework.name</name>
  <value>yarn</value>
</property>

<property>
  <name>mapreduce.application.classpath</name>   
  <value>$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*</value>
</property>

<!--추가-->
<property>
  <name>yarn.app.mapreduce.am.env</name>
  <value>HADOOP_MAPRED_HOME=$HADOOP_HOME</value>
</property>

<property>
  <name>mapreduce.map.env</name>
  <value>HADOOP_MAPRED_HOME=$HADOOP_HOME</value>
</property>

<property>
  <name>mapreduce.reduce.env</name>
  <value>HADOOP_MAPRED_HOME=$HADOOP_HOME</value>
</property>
```



* #5 **yarn-site.xml**: configuration 추가

```xml
<property>
		<name>yarn.nodemanager.aux-services</name>
		<value>mapreduce_shuffle</value>
</property>
<property>
    <name>yarn.nodemanager.env-whitelist</name>
    <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME</value>
</property>

<!--추가-->
<property>
  <name>yarn.resourcemanager.hostname</name>
  <value>127.0.0.1</value>
</property>
```

---

##  실행

* 실행전 준비

  ```shell
  ssh localhost 
  
  # 실행이 안된다면
  ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa
  cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
  chmod 0600 ~/.ssh/authorized_keys
  
  # 환경설정 >> 공유 >> 원격 로그인 켬
  ```

* hdfs 포맷

  ```shell
  cd /usr/local/cellar/hadoop/3.3.1/libexec/bin
  hdfs namenode -format
  ```

* 하둡 실행

  ```shell
  cd /usr/local/cellar/hadoop/3.3.1/libexec/sbin
  ./start-all.sh
  # 또는
  ./start-dfs.sh
  # 또는
  ./start-yarn.sh
  ```

* 실행중인 파일 확인

  ```shell
  jps
  # 28866 ResourceManager
  # 33716 NameNode
  # 34325 Jps
  # 34232 NodeManager
  # 33816 DataNode
  # 33950 SecondaryNameNode
  ```

* localhost 접속

  > `http://localhost:8088`: cluster
  > `http://localhost:9870`: core-site
  > `http://localhost:9860`: **name node**
  > `http://localhost:9850`: secondary node

* [tip] 단축어 등록

  : sbin에 cd할 필요 없이 hstart와 hstop으로 hdfs를 실행/중단 할 수 있음

  ```shell
  echo alias hstart="/usr/local/Cellar/hadoop/3.3.1/libexec/sbin/start-all.sh" >> ~/.zshrc
  
  echo alias hstop="/usr/local/Cellar/hadoop/3.3.1/libexec/sbin/stop-all.sh" >> ~/.zshrc
  ```

---

## 명령어

```shell
# vm을 생각해보자
hdfs dfs -mkdir /test00
hdfs dfs -ls /
hdfs dfs -mkdir /test01
hdfs dfs -ls

# local to hdfs
hdfs dfs -put ./test.txt /test00
hdfs dfs -ls -R / # 하위 목록 까지 표현
# cat
hdfs dfs -cat /test00/test.txt
# touch
hdfs dfs -touchz /test01/hello.txt
```

---

```shell
hadoop jar /usr/local/Cellar/hadoop/3.3.1/libexec/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.1.jar wordcount /input01 /output01

hadoop jar /usr/local/Cellar/hadoop/3.3.1/libexec/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.1.jar wordcount /input01 /output01

# 에러
conf.Configuration: resource-types.xml not found
resource.ResourceUtils: Unable to find 'resource-types.xml'.
```

---

## VM Hadoop 설치

* 기본 사양
  * Oracle VM 6.1
  * Centos 8.0 VM
  * VM: HadoopSvr00
    * java: jdk 1.8+
    * Hadoop v3.2.2
* 주의점
  * network 설정
  * 인증, 권한
  * hostname 설정
  * 환경변수 설정
* 참고 사이트
  * [How To Install and Configure Hadoop on CentOS/RHEL 8](https://tecadmin.net/install-hadoop-centos-8/)

---

## [Advenced] Docker Hadoop

* [[Docker\] Docker로 Hadoop 구성하기 #1 - MacOS에 Docker 설치 및 CentOS 실행](https://taaewoo.tistory.com/entry/Docker-Docker로-Hadoop-구성하기-1-MacOS에-Docker-설치-및-CentOS-실행)
