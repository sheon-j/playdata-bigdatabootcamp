# DEB_058

## Flume

```sh
# haddop@hadoop@00

# flume 설치
wget
https://dlcdn.apache.org/flume/1.9.0/apache-flume-1.9.0-bin.tar.gz

tar -xvzf apache-flume-1.9.0-bin.tar.gz

mv apache-flume-1.9.0-bin ../flume


# 환경설정
vi .bashrc
export FLUME_HOME=/home/hadoop/flume
export PATH=$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/bin:$HIVE_HOME/bin:$SQOOP_HOME/bin:$FLUME_HOME/bin
source .bashrc

# FLUME NG 기본 명령어
flume-ng version
flume-ng help

# $FLUME_HOME/conf/
cat flume-env.sh.template
cp flume-env.sh.template flume-env.sh
vi flume-env.sh
>> export JAVA_HOME=$JAVA_HOME # 추가

# 로그 접근을 위한 권한부여
su root
cd /var/log
ll secure
chmod o+r secure
exit

# agent 설정: $FLUME_HOME/conf
## flume-ng 에이전트 -n 이름 -c 위치 -f 파일명
cp flume-conf.properties.template flume-conf-00.properties

cat > flume-conf-00.properties
##############################
a00.sources = s00
a00.channels = c00
a00.sinks = k00

a00.sources.s00.type = exec
a00.sources.s00.command = tail -F /var/log/secure
a00.sources.s00.channels = c00
a00.sources.s00.interceptors=i00
a00.sources.s00.interceptors.i00.type=timestamp

a00.sinks.k00.type=hdfs
a00.sinks.k00.channel=c00
# 저장경로 / 방식
a00.sinks.k00.hdfs.path=hdfs://192.168.56.100:9000/user/flume/events/%y-%m-%d/%H%M/
a00.sinks.k00.writeFormat=Text

a00.channels.c00.type = memory
a00.channels.c00.capacity = 1000
a00.channels.c00.transactionCapacity = 100
##############################

# 라이브러리 설정: 
## 1. 에러이슈: $FLUME_HOME/lib
ll guava* # guava-11.0.2.jar 뜬다면 guava 버전 이슈
mv guava-11.0.2.jar __guava-11.0.2.jar
cd $HADOOP_HOME/share/hadoop/common/lib/
cp guava-27.0-jre.jar $FLUME_HOME/lib/
cp woodstox-core-5.0.3.jar $FLUME_HOME/lib/
cd $FLUME_HOME/lib
ls guava* && ls wood*
## 2. 연결 거부 이슈: $HADOOP_HOME/etc/hadoop/core-site.xml
hdfs://hadoop00:9000
> hdfs://192.168.56.100:9000

# 실행
## 실행문은 flume-ng agent -n a00 -c conf -f conf/flume-00.properties 의 형태
flume-ng agent --conf $FLUME_HOME/conf/ --conf-file $FLUME_HOME/conf/flume-conf-00.properties --name a00 -Dflume.root.logger=INFO,console

# 접속 로그 생성
## 1
while [ 1 -le 2 ] ; do ssh hadoop00 hostname; done
## 2
x=0; while [ $x –le 100 ]; do ssh hadoop00 hostname; x=$((x+1)) ; done

# hdfs에서 flume을 통해 저장된 데이터 확인
hdfs dfs -ls -R /user/flume/events/		# 날짜폴더/시간폴더/데이터로그
```

* 참조
  *  https://flume.apache.org/releases/content/1.9.0/FlumeUserGuide.html
  * https://www.jopenbusiness.com/mediawiki/Flume
  * http://hochul.net/blog/apache-flume-data-collector_2/
  * https://deep-jin.tistory.com/entry/Apache-Flume-기본-개념-정리?category=1104432?category=1104432

---

## Spark

* VM 메모리 설정: 8192 / CPU 설정: 4
* 참고
  * [SPARK 공홈](https://spark.apache.org)
  * [SPARK 다운로드](https://www.apache.org/dyn/closer.lua/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz)
  * [Wikidocs 스파크 구성](https://wikidocs.net/28377)
  * [스파크 예제](https://spark.apache.org/examples.html)
* $SPARK_HOME 구성
  * bin: spark를 다루는 언어와 관련된 프로그램 디렉토리
  * sbin: spark의 아키텍쳐와 관련된 프로그램 디렉토리

### Spark Install

```shell
# resource 확인
free -g													# 가용 메모리 확인
grep -c process /proc/cpuinfo   # 가용 cpu 확인


# 설치
wget https://dlcdn.apache.org/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz

tar -xvzf spark-3.1.2-bin-hadoop3.2.tgz

mv spark-3.1.2-bin-hadoop3.2 /home/hadoop/spark


# 환경변수설정
cd; vi .bashrc
>> export SPARK_HOME=/home/hadoop/spark
>> export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin
source .bashrc


# 스파크 확인
echo $SPARK_HOME
spark-submit --version
spark-submit --help


# hadoop 구동
su hadoop
start-all.sh


# SPARK 구동
spark-shell # scala
scala> :help
scala> :quit

spark-sql		# sql

pyspark			# pyspark
>>> help() ^C
>>> exit()


# SPARK 환경설정(worker node): $SPARK_HOME/conf
su root
cp spark-env.sh.template spark-env.sh
vi spark-env.sh
>> export SPARK_WORKER_INSTANCES=3	# Worker Node 할당

# SPARK Master Node 실행
start-master.sh
jps | grep Master  # Master Node가 올라옴
## http://192.168.56.100:8080 접속 성공!
## spark://hadoop00:7077 라고 쓰여짐

# SPARK Worker Node 실행
free -g													# 가용 메모리 확인
grep -c process /proc/cpuinfo   # 가용 cpu 확인
## resource 확인 후 메모리(-m) CPU(-c) 파라미터 할당
sh start-slave.sh spark://hadoop00:7077 -m 1g -c 1
jps | grep Worker		# Worker Node가 세대가 올라옴
## http://192.168.56.100:8080 접속 시 worker node 구성 나옴
```

### Spark 예제 실습

```shell
cd $HIVE_HOME
# 워드카운트 타겟 파일
cat README.md

# Master Node로 접속
spark-shell --master spark://hadoop00:7077
scala> val lines = sc.textFile("README.md")
scala> lines.count()
# http://192.168.56.100:8080 확인
# http://192.168.56.100:4040 실행 결과
```

* in-memory
* master - worker
* RDD, DataFram, DataSet

HiveConf: HiveConf of name hive.distcp.privileged.doAs does not exist

spark-sql 실행시 com.mysql.jdbc.Driver 드라이버가 없다고 에러가 뜨는데 따로 차리를 해야하나요?

---

## 과제

>  [과제] 2022.02.08 정승헌.spark
>
> 1. 조사 : Spark RDD, DataFrame, DataSet
> 2. 조사 : Scalar 언어란 ?
